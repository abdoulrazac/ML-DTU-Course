{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Data Mining - Part 3: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web crawling and scraping represent a very flexible way to get the content from the Internet. Essentially, it imitates a user who visits different webpages and views their content. The only difference is that Internet companies usually love real users and hate scraping bots. So be prepared to be blocked. **In the worst case, you can get into serious troubles so please always read Terms & Conditions and follow the company's policy about automatic data collection (or ask them directly if you are not sure)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement our web scraper. However, if you want to do it properly, just use one of the many frameworks available, for example, Scrapy (https://scrapy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get some data from the DTU website. Particularly, we want to get news headlines together with the date and short description from https://www.dtu.dk/english/news. Hopefully, we will not get banned or sued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Navigating the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the webpage shows 10 recent news items. The first task is to find out the URL structure to automatically navigate through the list of news, e.g. how to get the next 10 news items, show 100 news items instead of 10, etc. Try to figure out how which parameters in the URL are responsible for what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.dtu.dk/english/news\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and print a list of URLs to get webpages which contain the first 200 news items. \n",
    "\n",
    "***WARNING! Just generate the list and do not access the content!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "urls = []\n",
    "n_items = 200\n",
    "\n",
    "# your code ...\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the webpage content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We will not do it.*** I already scrapped a webpage with news items and saved it using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import urllib.request\n",
    "url = \"https://www.dtu.dk/english/news?fr=101&mr=100\"\n",
    "contents = urllib.request.urlopen(url).read()\n",
    "with open(\"dtu_news.html\", \"wb\") as f:\n",
    "    f.write(contents)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that all markup and images are missing since we saved only the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dtu_news.html\", \"r\") as f:\n",
    "    contents = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task to extract the news items from the contents using either regular expressions or HTML parser (e.g., PyQuery, BeautifulSoup) and save them to a JSON file.\n",
    "\n",
    "*HINT: To better understand the structure of the webpage, try:*\n",
    "- Chrome: right click -> inspect\n",
    "- Safari, Firefox, Edge: right click -> inspect element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# the output should be the following\n",
    "news = [\n",
    "    ...,\n",
    "    {\n",
    "        'url': 'https://www.dtu.dk/english/news/Nyhed?id={B4C5541F-8E2F-4118-9DA5-07A2BA6E2407}',\n",
    "        'title': 'New study underlines sea level rise in the Arctic ocean',\n",
    "        'desc': 'Sea levels in the Arctic oceans have risen an average of 2.2 millimeters per year over the last 22 years.This is the conclusion reached by a Danish-German research team...',\n",
    "        'date': '16 JUL'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
