{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on the data from MovieLens and the [notebook](https://www.kaggle.com/devvindan/recommender-systems-basics) by Daniil Barysevich*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fbc0acd43a26815fe27afc079b99f491c8d8e05"
   },
   "source": [
    "The purpose of this exercise is to explore the basics of Recommender Systems and to give you some intuition with code examples. It covers some popular algorithms and strategies but does not get deeply into advanced techniques or evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8866ada27332019d8cbe2cad877f0fa50efed1d3"
   },
   "source": [
    "### Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Non-Personalized Recommender Systems\n",
    "3. Personalized Recommender Systems   \n",
    " 3.1. Content-Based Filtering  \n",
    " 3.2. Collaborative Filtering\n",
    "``` \n",
    "      3.2.1. User-user collaborative filtering\n",
    "      3.2.2. Item-item collaborative filtering\n",
    "``` \n",
    " 3.3. Matrix factorization  \n",
    " 3.4. Hybrid Recommender Systems\n",
    "4. Playtime: Bringing all together\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b85d25b3457b521cd43db2f090158d285da1f83"
   },
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79623471cc19fde8c4a050da7795a7c741406b9c"
   },
   "source": [
    "(some background information and Wikipedia text)\n",
    "\n",
    "A *recommender system* or a *recommendation system* (sometimes replacing \"system\" with a synonym such as a *platform* or an *engine*) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.\n",
    "\n",
    "Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners (online dating), and Twitter pages.\n",
    "\n",
    "Recommender systems typically produce a list of recommendations in one of two ways:\n",
    " - Non-personalized approach\n",
    " - Personalized approach\n",
    " \n",
    "Personalized-based recommender systems also have common subtypes:\n",
    " - Collaborative filtering recommender systems\n",
    " - Content-based recommender systems\n",
    " \n",
    "All the techniques mentioned above have their problems and pitfalls, which developers face creating and applying recommender systems to real-world problems. They must be taken into account while designing the system architecture and will be covered later in this work. Though the field of recommendation itself is relatively old, there are still no solutions that work perfectly for every case. Designing and evaluating a recommender system is hard, and requires a deep understanding of domain knowledge and data available, as well as constant experimenting and modification. First recommender systems appeared a long time ago in the 1990s, but the intense research started quite recently with the availability of better computational power and tremendous amounts of data coming from the internet.\n",
    "\n",
    "One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system.\n",
    "\n",
    "This competition energized the search for new and more accurate algorithms. The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction:\n",
    "\n",
    ">Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique. Consequently, our solution is an ensemble of many methods.\n",
    "\n",
    "Beside classical approaches to recommendation with techniques described above, there are a lot of different cases that require modifications or special settings:\n",
    " - Group recommender systems\n",
    " - Context-aware recommender systems\n",
    " - Risk-aware recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c394b9d149668fff1c26453f4dca04a134f5cc5"
   },
   "source": [
    "### 2. Non-personalized Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b21b7926e663c846aa54c5766040612196f9cbbc"
   },
   "source": [
    "Though non-personalized recommenders are rarely used in modern systems by themselves, they are still very powerful in combination with other algorithms, and, sometimes, the only available option. \n",
    "\n",
    "How can we make a recommendation for a user that we have little or no data about? \n",
    "\n",
    "That's where stereotype-based recommendations can be made, and most of the times we can take into account:\n",
    " - items popularity\n",
    " - user demographic data \n",
    " - user actions during that particular session (for example, items in online-shop basket)\n",
    " \n",
    "**Mean-based recommendation:** \n",
    "\n",
    "One of the common approaches we can use is a mean-based recommendation. In the simplest case, we can use mean rating for the item, $\\mu_i$, to recommend items with the highest rating\n",
    "\n",
    "$$\\mu_i = \\frac{\\sum_x r_{xi}}{N_{i}}$$\n",
    "\n",
    "where $r_{xi}$ is the rating for the item $i$ by the user $x$ and $N_{i}$ is the number of ratings for the item.\n",
    "\n",
    "**Associative rule recommendation:**\n",
    "\n",
    "This approach is used to recommend items that are related to chosen one (\"People who buy this also bought...\") and, therefore, uses *reference item* to provide recommendations.\n",
    "\n",
    "The association rule formula is derived from Bayes theorem:\n",
    "\n",
    "$$P(i|j) = \\frac{P(i \\vee j)}{P(j)}$$\n",
    "\n",
    "In this case, *j* is the *reference item*, and *i* is an item to be scored. We estimate probabilities by counting: $P(j)$ is the fraction of users in the system who\n",
    "purchased item $j$; $P(i\\vee j)$ is the fraction that purchased both $i$ and $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "The following tasks cover some examples of non-personalized data analysis based on the MovieLens dataset. Please note that there are several different data files used in this notebook for different tasks. For this task, we will use the data from `data/HW1-data.csv`. Please load the data from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63581f8aefde8f4f58be460e6d81eb5e813f917b"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find the range of ratings (minimum and maximum values) in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be the following:\n",
    "```\n",
    "Minimum rating: 1.0\n",
    "Maximum rating: 5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "695c1a6390dff4d02389c0add6fba6287b1f4aa2"
   },
   "source": [
    "Let's calculate top movies by their mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bb01e4eb40e223559f68e7808270202bbb2bce5"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07c6f182b5d667cfb116441402ecafa7ba89b568"
   },
   "source": [
    "...and rating counts for each movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3f56b112af00861003013527239056365c4b5c7"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73aafc43f75c5d0e157893a71030e0921db51875"
   },
   "source": [
    "Sometimes, we do not need to know a precise rating to make a recommendation. Therefore, we can define some ratings as positive (for example, all the ratings >= 4). Print top movies by the percentage of positive marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee4a105b21c68298d0a2add3febb3e32babadfc1"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d62b87524108981a453045a0ffeadb3fae182dab"
   },
   "source": [
    "Let's imagine we watched the movie \"Toy Story\" and we want to have a list of relevant movies to watch next. We can apply association rule here. Range movies by the percentage of people who also watched Toy Story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "accfb8207e27b1edd0f2699253d17737c180bfa8"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc632c859c055a903ed1b29c8dc9ab8e68fb733e"
   },
   "source": [
    "Making recommendations above, we did not use data about the user's gender. Statistically, men and women tend to like or dislike different kinds of movies, so, to make non-personalized recommendations more precise, we can take this information into account and see the difference. Find and print mean ratings for each movie as well as global mean ratings for male and female users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b2c2e57036735a8f9b00ed5face2f9a7d6f03aa"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find movies that female users rate higher than male raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e2906b0c832005206042d8dd2177e6c5475c600"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a84b88972f080711baa8dc1597007ecdc7cd83df"
   },
   "source": [
    "### 3. Personalized Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "043b4d66e9792791e92b9050280688a636d19974"
   },
   "source": [
    "All the personalized recommendation require a certain amount of data collected about users. Data could either be collected implicitly (products user click on, see) and explicitly (in forms of ratings, surveys, polls). Both methods are used widely and can be combined depending on the system restrictions and type of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72ca2f80630349238d107f9f8c57ec81d4c94f53"
   },
   "source": [
    "#### 3.1 Content-based filtering (linear model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0513e19f308f190b97508323f8ec2b734b945c7"
   },
   "source": [
    "Content-based filtering, also referred to as cognitive filtering, recommends items based on a comparison between the content of the items and a user profile. The content of each item is represented as a set of descriptors or terms, typically the words that occur in a document. The user profile is represented with the same terms and built up by analyzing the content of items which have been seen by the user.\n",
    "\n",
    "Several issues have to be considered when implementing a content-based filtering system. First, terms can either be assigned automatically or manually. When terms are assigned automatically a method has to be chosen that can extract these terms from items. Second, the terms have to be represented such that both the user profile and the items can be compared in a meaningful way. Third, a learning algorithm has to be chosen that can learn the user profile based on seen items and can make recommendations based on this user profile.\n",
    "\n",
    "The greatest advantage in \"linear\" content-based filtering systems is that the recommendations provided can easily be interpreted to the user because we always know what \"features\" about a particular item made algorithm rate it higher.\n",
    "\n",
    "The example of content-based filtering applied to news recommendation based on terms contained in a document. Import the raw data from the excel file `data/cbf.xls`. To do this, the Pandas function `pd.read_excel(file)` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2bd587a69864d3d19a633472d540a92988d6421"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(\"data/cbf.xls\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains topics for the documents and ratings of two users. Let's create two separate dataframes for these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6334a508346d3b6dc9954cc8b0735833ad66b1a9"
   },
   "outputs": [],
   "source": [
    "docs = raw_data.loc['doc1':'doc20', 'baseball':'family']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1019b0778f533ef08708cf86d9cd4a72cd504feb"
   },
   "outputs": [],
   "source": [
    "user_ranks = raw_data.loc['doc1':'doc20', 'User 1':'User 2']\n",
    "#user_ranks.fillna(0, inplace=True)\n",
    "user_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc735286251643d503ce020bb25bcf943d26adc5"
   },
   "source": [
    "The value of 1.0 means the user liked the document, the value of -1.0 - disliked. NaN means that the user has never seen the document (and we have to predict rating).\n",
    "\n",
    "In the simplest case, we can learn user profiles using logistic regression corresponding to the binary problem - the probability of an item being liked by the user. Let's train a separate logistic regression model for each user. Do not forget to re-assign 0 to \"dislike\" values instead of -1 to make the logistic regression applicable. As usual, you can use `sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the coefficients of the regression. What you can tell about each user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range the documents by the probability of being liked by the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "065878b003b1a3c5e1fc52eb9bc658cfe06c101d"
   },
   "source": [
    "#### 3.2 Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "813df0a0d1ed87cd822b1d14806396cce088d62a"
   },
   "source": [
    "Collaborative filtering, also referred to as social filtering, filters information by using the recommendations of other people. It is based on the idea that people who agreed in their evaluation of certain items in the past are likely to agree again in the future. A person who wants to see a movie, for example, might ask for recommendations from friends. The recommendations of some friends who have similar interests are trusted more than recommendations from others. This information is used in the decision on which movie to see.\n",
    "\n",
    "Collaborative filtering often uses the concept of **neighborhood** (the amount of people/items we base our prediction on). Making neighborhoods too small results in not enough information for accurate prediction, and making them too big results in high computational complexity and letting noize in systems. Neighborhood size is a hyperparameter which needs to be tuned in every system. Distance between neighbors can be defined using such metrics as cosine similarity.\n",
    "\n",
    "One of the most common problems all collaborative filtering recommender systems face - a so-called \"cold start\" problem, when we either:\n",
    " - do not have enough ratings for a new user to find neighbors\n",
    " - do not have enough ratings for a new item to find neighbors\n",
    " - have a completely new system without any data to make recommendations\n",
    " \n",
    "In each of those cases, problems might be solved differently depending on the particular case and options available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53e0cd158cb503689babbb3a7493f7a8b86d8b25"
   },
   "source": [
    "##### 3.2.1 User-user collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d86f703c881b1ab75122cd47f411f29bd3b542a"
   },
   "source": [
    "In user-user collaborative filtering, we provide a recommendation based on tastes of other users similar to us. The problem with that algorithm is that we need a lot of information about other people to provide correct recommendations, but the main benefits are effectiveness and ability to provide new, unexpected, and, yet, good recommendations.\n",
    "\n",
    "In order to account for user's tendecy to give higher/lower ratings, we will use normalization again. Algorithm for providing score based on user-user collaborative filtering is defined as:\n",
    "\n",
    "$$ r_{xi} = \\mu_{x} + \\frac{\\Sigma_{y \\in \\mathbf{K}}sim(x,y)(r_{yi}-\\mu_{y})}{\\Sigma_{y \\in \\mathbf{K}}|sim(x,y)|}  $$\n",
    "\n",
    "Where $\\mu_x$ is a mean rating for a user and $sim$ can be defined in different ways, for example, as cosine similarity or correlation.\n",
    "\n",
    "For this example, we will use the data from the `data/data.xls` file. Again, to read the file, the pandas function `pd.read_excel(file)` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64a8f7e8d12dddf848814c8da746555bfe924d58"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a correlation matrix between different users. Note, that the pandas' function `corr()` ignores missing values so it does the extra job to find overlapping rating sets for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlations = data.transpose().corr()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ef3f69228961cd2000f4ef1b6eafb5906051afe"
   },
   "source": [
    "For this example, we will make predictions for user 3867. Our 'neighborhood' for a user - users with $K=5$ highest correlations. Find these users and print their ratings and correlations to user 3867."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "user_id = 3867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d52aaae467f8ec5f9bc9c2901a946de1c62b9d9"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find the top 5 movies to recommend to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like the following:\n",
    "```\n",
    "77: Memento (2000)                               4.777803\n",
    "275: Fargo (1996)                                4.771538\n",
    "807: Seven (a.k.a. Se7en) (1995)                 4.655569\n",
    "194: Amelie (2001)                               4.449936\n",
    "63: Twelve Monkeys (a.k.a. 12 Monkeys) (1995)    4.396449\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c548173995ef2c02e8a46ae64951891ae69d1b7c"
   },
   "source": [
    "##### 3.2.2 Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2f6bc62fdbcb5010080aafa075e25ef02bba3d2"
   },
   "source": [
    "In item-item collaborative filtering, we provide a recommendation based on other items similar to us. The benefits of it, compared to user-user collaborative filtering, is that we usually need much fewer similarity computations (in most cases, there are much more users in systems than items). The most common pitfall - the system can provide very obvious recommendations.\n",
    "\n",
    "Score provided by item-item filtering is computed using the following formula:\n",
    "\n",
    "$$ r_{xi} = \\mu_{i} + \\frac{\\Sigma_{j \\in \\mathbf{K}}sim(i,j)(r_{xj}-\\mu_{j})}{\\Sigma_{j \\in \\mathbf{K}}|sim(i,j)|}  $$\n",
    "\n",
    "Where $\\mu_i$ is a mean rating for an item and $sim$ can be defined in different ways, for example, as cosine similarity or correlation.\n",
    "\n",
    "Let's skip the visualization part and directly do the same top 5 predictions for the same user using the item-item algorithm. Note, that now we need to find new nearest neighbors for each movie to make this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "user_id = 3867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlations = data.corr()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look the following:\n",
    "```\n",
    "77: Memento (2000)                                             4.640297\n",
    "807: Seven (a.k.a. Se7en) (1995)                               4.481565\n",
    "146: Crouching Tiger Hidden Dragon (Wo hu cang long) (2000)    4.478644\n",
    "153: Lost in Translation (2003)                                4.415375\n",
    "194: Amelie (2001)                                             4.207079\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the recommendations using both algorithms are similar. However, in practice, the item-item algorithm produces better recommendations as it is easier to capture the similarity between items rather than users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "183824d5f74b258fa46d23d06e1cc798eb4d2791"
   },
   "source": [
    "#### 3.3 Matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46581d3e1ed180f0cc8f08f929958000b7aa63d6"
   },
   "source": [
    "In matrix factorization techniques, we usually represent the rating matrix as a product of 3 other matrices.\n",
    "\n",
    "$$R = P\\Sigma Q^{T}$$\n",
    "\n",
    "The benefits of those techniques are that they can dramatically improve system performance by reducing the necessary amount of space. Collaborative techniques can be later applied on decomposed matrices. \n",
    "\n",
    "`surprise` is an easy-to-use Python package for recommender systems. Please refer to their [project page](http://surpriselib.com/) and [document page](http://surprise.readthedocs.io/en/stable/index.html#) for details of installation and tutorials. Here I will use the famous [SVD algorithm](http://sifter.org/simon/journal/20061211.html). The document for this model in the Surprise page is [here](http://surprise.readthedocs.io/en/stable/matrix_factorization.html)\n",
    "\n",
    "We will do the same exercise as in the previous subsections and try to recommend movies to User 3867."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise.dataset import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, the data for the `surprise` package should be represented in the following format: \n",
    "```\n",
    "userID, movieID, rating\n",
    "```\n",
    "Let's create a dataframe for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/data.xls\")\n",
    "user_ids, movie_ids, ratings = [], [], []\n",
    "#\n",
    "for user_id, row in data.iterrows():\n",
    "    for movie_id in data.columns:\n",
    "        rating = row[movie_id]\n",
    "        if not np.isnan(rating):\n",
    "            user_ids.append(user_id)\n",
    "            movie_ids.append(movie_id)\n",
    "            ratings.append(rating)\n",
    "ratings_df = pd.DataFrame({'userID':user_ids, 'movieID':movie_ids, 'rating':ratings})\n",
    "ratings_df = ratings_df[['userID', 'movieID', 'rating']] # correct order\n",
    "ratings_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a dataset in the required format for the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rating scale should be specified in Reader\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data_surprise = Dataset.load_from_df(ratings_df, reader)\n",
    "data_train_surprise = data_surprise.build_full_trainset() # use the whole training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the SVD algorithm. Read [documentation](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 15\n",
    "lr_all = 0.005 # default value\n",
    "reg_all = 0.02 # default value\n",
    "#\n",
    "model = SVD(n_factors=n_factors, lr_all=lr_all, reg_all=reg_all)\n",
    "model.fit(data_train_surprise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will make the top 5 predictions for User 3867."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 3867\n",
    "missing_movies_ids = data[data.isna()].loc[user_id].index\n",
    "#\n",
    "recommendations = []\n",
    "for movie_id in missing_movies_ids:\n",
    "    r = model.predict(user_id, movie_id, verbose=False).est\n",
    "    recommendations.append((movie_id, r))\n",
    "    #print(movie_id)\n",
    "    #break\n",
    "recommendations.sort(reverse=True, key=lambda x: x[1])\n",
    "recommendations[:10]\n",
    "#pred = model.predict(rating_train.userID[i], rating_train.placeID[i], verbose=False)\n",
    "#r_pred_train[i] = pred.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations are now quite different from the previous two algorithms. To estimate performance of the different approaches, we would need to do train/test split (or cross-validation). We will skip this comparison here but you can try to do it yourself if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2ab7b73b225d31b597aafe5b3f17791155a9f80"
   },
   "source": [
    "#### 3.4 Hybrid recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a90cf4a7d5761fc46d230b4a575a3b90b1a2514"
   },
   "source": [
    "In hybrid recommender systems, recommendations are made usually based on scores provided by multiple recommender systems. The most common technique is to represent the final score as a linear combination of scores provided by other recommenders with according weights. \n",
    "\n",
    "Another option is a so-called \"switch\" recommender system. Given some input, the system decides, which of the available recommender engines is better to use for a recommendation in this particular situation. Such an algorithm helps to overcome problems that exist in each recommender separately.\n",
    "\n",
    "We also can use so-called \"cascade\" hybrid recommenders - the system where outputs of one recommendation algorithm are used as inputs to other. \n",
    "\n",
    "There are dozens of ways to use hybrid recommender systems, and there is no common way of applying them to a real-world problem. Design and architecture of each of such systems depend on data available, domain field and requirements for a particular system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Playtime: Bringing all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to build a recommender system based on SVD and evaluate its performance against the common baselines. We will use the same data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preparing the dataset\n",
    "data = pd.read_excel(\"data/data.xls\")\n",
    "#data\n",
    "user_ids, movie_ids, ratings = [], [], []\n",
    "#\n",
    "for user_id, row in data.iterrows():\n",
    "    #print(row)\n",
    "    for movie_id in data.columns:\n",
    "        #print(movie_id, user_id)\n",
    "        rating = row[movie_id]\n",
    "        if not np.isnan(rating):\n",
    "            #print(movie_id, user_id, row[movie_id])\n",
    "            user_ids.append(user_id)\n",
    "            movie_ids.append(movie_id)\n",
    "            ratings.append(rating)\n",
    "ratings_df = pd.DataFrame({'userID':user_ids, 'movieID':movie_ids, 'rating':ratings})\n",
    "ratings_df = ratings_df[['userID', 'movieID', 'rating']] # correct order\n",
    "#ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Data and Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 70%/% 30 train/test split. The following code, which guarantees that each movie appears at least twice in the training set, was used to create this split. Everything is pre-calculated for you, so you do not need to run this code again. The train and test indices (here, I mean actual row numbers from the data table) are loaded from the files so everyone has the same data split and can directly compare results with other students."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_ind, test_ind = [], []\n",
    "#\n",
    "for movie_id in ratings_df['movieID'].unique():\n",
    "    rows = ratings_df[ratings_df['movieID'] == movie_id]\n",
    "    ind = rows.index[:2].values.tolist()\n",
    "    train_ind += ind\n",
    "#\n",
    "c = 0.7\n",
    "b = len(train_ind) / len(ratings_df)\n",
    "a = (c - b) / (1 - b)\n",
    "print(a)\n",
    "#\n",
    "all_ind = set(range(len(ratings_df)))\n",
    "not_used = list(all_ind - set(train_ind))\n",
    "not_done = True\n",
    "np.random.seed(42)\n",
    "while not_done:\n",
    "    np.random.shuffle(not_used)\n",
    "    train_ind_ = train_ind + not_used[:int(a * len(not_used))]\n",
    "    df_train = ratings_df.loc[train_ind_]\n",
    "    print(ratings_df.nunique()['movieID'], df_train.nunique()['movieID'])\n",
    "    print(ratings_df.nunique()['userID'], df_train.nunique()['userID'])\n",
    "    if ratings_df.nunique()['movieID'] == df_train.nunique()['movieID'] and ratings_df.nunique()['userID'] == df_train.nunique()['userID']:\n",
    "        not_done = False\n",
    "        train_ind = train_ind_\n",
    "#\n",
    "test_ind = list(all_ind - set(train_ind))\n",
    "#\n",
    "train_ind = sorted(train_ind)\n",
    "test_ind = sorted(test_ind)\n",
    "print(len(train_ind)/len(all_ind))\n",
    "print(len(test_ind)/len(all_ind))\n",
    "#\n",
    "np.savetxt('data/data_train.csv', train_ind, fmt=\"%d\")\n",
    "np.savetxt('data/data_test.csv', test_ind, fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pre-computed indices\n",
    "train_ind = np.loadtxt('data/data_train.csv', dtype=int)\n",
    "test_ind = np.loadtxt('data/data_test.csv', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training samples:\", train_ind.shape[0])\n",
    "print(\"number of test samples:\", test_ind.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test dataframes can be created using the `iloc` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ratings_df.iloc[train_ind]\n",
    "df_test = ratings_df.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 4 baseline models:\n",
    "1. Global mean\n",
    "2. User's mean\n",
    "3. Movie's mean\n",
    "4. Global + deviation of user + deviation of the movie\n",
    "\n",
    "Estimate them on the training set and calculate RMSE on the test set. Also, make boxplots for the visualization of the predictions. The code for the global mean baseline is given as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.linalg.norm(y_true - y_pred) / np.sqrt(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plotter to make boxplot\n",
    "def MakeBoxplot(y_true, y_pred, title):\n",
    "    data = [y_pred[y_true == (x*0.5+0.5)] for x in range(10)]\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.boxplot(data)\n",
    "    min_a, max_a = 0., 5.5\n",
    "    plt.xlim((min_a, max_a))\n",
    "    plt.ylim((min_a, max_a))\n",
    "    plt.plot([min_a, max_a * 2], [min_a, max_a], ls='--', color='gray', linewidth=1.0)\n",
    "    plt.xticks(range(12), [x*0.5 for x in range(12)])\n",
    "    plt.xlabel('True Rating')\n",
    "    plt.ylabel('Predicted Rating')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_test['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global mean\n",
    "global_mean = df_train['rating'].mean()\n",
    "print(\"global_mean =\", global_mean)\n",
    "# prediction\n",
    "y_pred = []\n",
    "for i, row in df_test.iterrows():\n",
    "    y_pred.append(global_mean)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.clip(y_pred, 0.5, 5.0)\n",
    "# or simply y_pred = np.array([global_mean for i in range(len(y_true))])\n",
    "# performance\n",
    "error = RMSE(y_true, y_pred)\n",
    "print(\"RMSE =\", error)\n",
    "MakeBoxplot(y_true, y_pred, 'Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user mean\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie mean\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Trying to beat the baselines with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we beat the baselines with SVD? Note, that SVD has many hyperparameters, where the number of factors (`n_factors`) and regularization strength (`reg_all`) are the most important. As is usual in machine learning, you are not allowed to tune them on the test set. Either use cross-validation on the training set or divide the training data into two sets: validation set (~30% of the training data) and actual training set used to fit parameters of the model. \n",
    "\n",
    "*Hint: You can use `surprise.model_selection.GridSearchCV` for the hyperparameter tuning. Check an example here: https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv*\n",
    "\n",
    "I managed to get RMSE = 0.85, can you do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data_surprise = Dataset.load_from_df(df_train, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1323cb1d8f5662b690060311597b81604455384"
   },
   "source": [
    "### 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52aa4c7a3c5029dce721e722ed8e46ec331bfc34"
   },
   "source": [
    "Building a good recommender system is not an easy task. Although some algorithms are considered \"best practices\", they all have their strengths and weaknesses. Developing a recommender system requires a good understanding of domain users, data that can be collected, and purposes of our recommendation. Without knowing all the things mentioned above, it is impossible to design a good recommender system, no matter how complicated are algorithms you use. \n",
    "\n",
    "However, there are common techniques that are commonly used by themselves and in combination. Modern recommender systems still use collaborative filtering and content filtering techniques, although nowadays these algorithms are used in combination and with the application of such more advanced techniques as matrix factorization, neural networks, and hybrid recommender systems.\n",
    "\n",
    "The field of recommender systems is constantly developing, providing us with new studies on context-based recommendation, risk-aware, and group recommendations, as well as research in different evaluation methods and iterative factorization techniques. There are dozens of ways to design a recommender, and choosing \"the best\" approach is up to people who know why and how they want to make recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
