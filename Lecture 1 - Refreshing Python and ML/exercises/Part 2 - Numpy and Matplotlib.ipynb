{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshing Python and Machine Learning: Part 2 - Numpy and Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on the notebooks from 42184 Data Science for Mobility E19 / 42577 Introduction to Business Analytics E19*\n",
    "\n",
    "*Some parts of this notebook was originally conceived by Prof. Sune Lehmann, for the Social data analysis and visualization (02806) course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algebra in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, you need to get comfortable with vector and matrix manipulation in Numpy, a very handy Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "\n",
    "You can read more about NumPy [here](https://docs.scipy.org/doc/numpy/user/whatisnumpy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why NumPy???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Memory efficiency:** NumPy's arrays are more compact than Python lists. For example, where a Python list would take at least 20 MB, a NumPy 3D array with single-precision floats in the cells would fit in 4 MB. Access in reading and writing items is also faster with NumPy.\n",
    "\n",
    "* **Conveniency:** You get a lot of vector and matrix operations for free, which sometimes allow one to avoid unnecessary work. And they are also efficiently implemented.\n",
    "\n",
    "* **Speed:** Here's a test on doing a sum over a list and a NumPy array, showing that the sum on the NumPy array is 10x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from timeit import Timer\n",
    "\n",
    "Nelements = 10000\n",
    "Ntimeits = 10000\n",
    "\n",
    "x = arange(Nelements)     #the \"new\" arange function in Python, creates an array with Nelements\n",
    "y = range(Nelements)      #the \"old\" one you know already, creates a list with Nelements\n",
    "\n",
    "t_numpy = Timer(\"x.sum()\", \"from __main__ import x\")    #a simple operation on the array created\n",
    "t_list = Timer(\"sum(y)\", \"from __main__ import y\")      #a similar operation but on a list\n",
    "print (\"numpy: %.3e\" % (t_numpy.timeit(Ntimeits)/Ntimeits,))\n",
    "print (\"list:  %.3e\" % (t_list.timeit(Ntimeits)/Ntimeits,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyâ€™s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers.\n",
    "\n",
    "<p>\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/elegant-scipy-1st/9781491922927/assets/elsp_0105.png\"/>\n",
    "</p>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Creating arrays\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([(1.5,2,3), (4,5,6)], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Placeholders \n",
    "(or ways to quickly create a vector/array with ones or zeros...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros([3,4])                    # Creates an 3x4 array of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2,3,4), dtype=np.int16)   # Creates a array of 2x3x4 ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10,30,5)                  # Creates an array of evenly spaced values (step value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,2,9)                 # Creates an array of evenly spaced values (number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full((2,2),7)                    # Creates an array of constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(2)                           # Creates a 2x2 identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random((2,2))             # Creates an array of random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   # we add this for you... \n",
    "#Yes, without this line below you'll run into trounble (wanna try?   ;-) )\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vector of 10000 samples from a normal distribution with variance 0.5^2 and mean 2\n",
    "mu, sigma = 2, 0.5   \n",
    "\n",
    "v = np.random.normal(mu,sigma,10000)   #get 100000 samples from a normal\n",
    "# Plot a normalized histogram with 50 bins\n",
    "plt.hist(v, bins=50, normed=1)       # matplotlib version (plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty((3,2))                     # Creates an empty array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "np.savetxt(\"myarray.txt\", a, delimiter=\" \")\n",
    "loaded_a=np.loadtxt(\"myarray.txt\")  #reads directly to an np.array\n",
    "print(loaded_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"myarray.txt\") #read it as if it was a normal text file\n",
    "for line in f.readlines():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing and Iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)**3   #power operator\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[2])\n",
    "print(a[2:5])\n",
    "print(a[ : :-1])      # reversed a\n",
    "\n",
    "for i in a:\n",
    "    print(i**(1/3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two dimensional arrays\n",
    "print(b)\n",
    "print(b[:1])       # prints the first row\n",
    "print(b[0:2,1])    # prints the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a<2]            # Selection of elements from \"a\" less than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)\n",
    "print(b.T) # Transposing Array (you could also do np.transpose(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing array Shape\n",
    "b=b.ravel()               # b becomes flatenned (all elements in a single row)\n",
    "print(b)\n",
    "b=b.reshape(2,3)     #let's get it back to the initial 2x3 form\n",
    "print(b)           #and here it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding/Removing elements\n",
    "c=np.append(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.insert(c, 3, 5) #insert number 5 in position 3 of array c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c) #before\n",
    "c=np.delete(c, 3) #removes the fourth element in array\n",
    "print(c) #after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining arrays\n",
    "np.concatenate((b,b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((b,b))   #Try hstack()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic numpy statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by downloading this dataset: [Data](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/data1.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "The file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download this file to disk by right-clicking on each one, but if you use Python and _urllib_ or _urllib2_ to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend.\n",
    "\n",
    "* Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values. \n",
    "* Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. \n",
    "* Now calculate the variance of $x$- and $y$-values (to three decimal places).\n",
    "* Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values (also to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numpy statistics and Matplotlib visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open the file \"data/pickups_zone_1_15min.csv\". This corresponds to the series of taxi-pickups in New York zone 1 (an area in the Manhattan island). \n",
    "\n",
    "You can use the function open(file), which **returns** a stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/pickups_zone_1_15min.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so we have the file stream, now we need to get to its content. This stream has a method to read all the lines at once (method readlines()). Let's use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let's check how many lines the file actually has (it should be 262849, if it's different, there's something wrong...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 lines of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so you have temporal attributes, and the actual number of pickps.\n",
    "\n",
    "So, print **only** the pickups part (10 first lines). \n",
    "\n",
    "A tip - look what the following code does:\n",
    "\n",
    "> x=\"1,2,3,4\"\n",
    "\n",
    ">xsplitted=x.split(',')\n",
    "\n",
    "\n",
    ">print(xsplitted)\n",
    "\n",
    ">print(xsplitted[2])\n",
    "\n",
    "Output:\n",
    "\n",
    "['1', '2', '3', '4']\n",
    "\n",
    "3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, our goal is thus to make a **single** list with all the pickup data. You need to go over each line, get the pickup value, convert it to an integer, and add it to this list. \n",
    "\n",
    "Can you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so now you have an actual time series dataset. As you'll shortly see, it's very useful to have the respective time stamps (not just know that it is a sequence of values, also know exactly **when** each value occured)\n",
    "\n",
    "Notice, however, that the values for the date are split into 3 fields. More importantly, for Python, they just numbers that have nothing to do with time. Luckily, there's class called datetime.datetime (the repetition here is intentional...). \n",
    "\n",
    "Here's some example code for you:\n",
    ">from datetime import datetime as dt\n",
    "\n",
    ">s=\"2017-09-11\"\n",
    ">\n",
    ">print(type(s))\n",
    ">\n",
    ">time=dt.strptime(s,  '%Y-%m-%d')\n",
    ">\n",
    ">print(type(time))\n",
    ">\n",
    ">print(time)\n",
    ">\n",
    ">time=time.replace(hour=14, minute=35)\n",
    ">\n",
    ">print(time)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "\n",
    "<class 'str' &gt;\n",
    "\n",
    "<class 'datetime.datetime' &gt;\n",
    "\n",
    "2017-09-11 00:00:00\n",
    "\n",
    "2017-09-11 14:35:00\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So, it creates a \"datetime\" object, that has everything you need to know about the time of that data point. It's pretty handy as you'll see later. \n",
    "\n",
    "Like what you did in the previous exercise, we now want a single list with all the datetime objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to reorganize your code a bit. You've made a few things above:\n",
    "- load a file\n",
    "- go over the content of the file to create a list with pickup data\n",
    "- go over the content of the file to create a list of datetime objects\n",
    "\n",
    "Let's put them together in a function that reads a file (with name fscv) and returns the two lists mentioned. I'll give you the first and last lines:\n",
    "\n",
    ">def read_csv(fcsv):\n",
    ">\n",
    ">      ...  #you just need to fill this part! ;-)\n",
    ">\n",
    ">      return pickups, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this function, you can run all the above with different files with a single command! Do you want to try?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks, times = read_csv(\"data/pickups_zone_1_15min.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create two numpy arrays from our two lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpicks = np.array(picks)\n",
    "vtimes = np.array(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickup vector can be used right away to make a histogram. Think about it: what should the distribution of number of pickups (i.e. the observable taxi demand) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches=plt.hist(vpicks, bins=60)\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"pickups\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hhmmm... was this what you were expecting?... \n",
    "\n",
    "Another interesting way to look at this data is simply by plotting directly with a scatter plot, where the x axis is the index of the datapoint, and the y axis is the total number of pickups.\n",
    "\n",
    "Tip: to see better the data try playing with the size of the dots (for example, put s=0.1 as an argument to the scatter call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(vpicks)), vpicks, s=0.1, alpha=0.5)\n",
    "plt.ylabel(\"# pickups\")\n",
    "plt.xlabel(\"Record ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of pattern is intriguing. There seems to be two general \"trends\" of taxi pickups in this area. Do you think it relates with time of day? It would make some sense (e.g. during the night, low values, during peak hours, high values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to check this, but a simple one is to make a small change in the temporal vector. Instead of each element of this vector corresponding to absolute time (the *actual* date and time), why not just represent the minutes since midnight? \n",
    "\n",
    "Can you make a new vector with that content?\n",
    "\n",
    "**Tip on how to get the hour and minute**:\n",
    ">from datetime import datetime as dt\n",
    ">\n",
    ">s=\"2017-09-11 18:35:11\"\n",
    ">\n",
    ">d=dt.strptime(s, \"%Y-%m-%d %H:%M:%S\")\n",
    ">\n",
    ">print(d)\n",
    ">\n",
    ">print(d.hour)\n",
    ">\n",
    ">print(d.minute)\n",
    ">\n",
    "\n",
    "Output:\n",
    "\n",
    "2017-09-11 18:35:11\n",
    "\n",
    "18\n",
    "\n",
    "35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now for a cool trick. In Python (well, in general), you ultimately define colors with numbers. So, imagine that the number of minutes since midnight (that you just created) corresponds to a color. The function scatter allows you to give this list straight away and plot it (just use the argument c, for example \"c=my_minute_since_midnight_list\". \n",
    "\n",
    "Do you want to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't this explain something?  ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there seems to be indeed a relationship with time! \n",
    "\n",
    "If this is true, it may be interesting to do a 24-hr average plot. In other words, a plot where the x axis is 0 to 1440 (1440=24 hours X 60 minutes), and you show the average per minute.\n",
    "\n",
    "BTW, it will also be very useful if you add the 5 and 95 quantiles. \n",
    "\n",
    "Can you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important task in Data Science modeling is to find (and understand) correlations between different variables. Let's do a few simple exercises.\n",
    "\n",
    "Let's start with a simple question: are the different areas correlated between them? If yes, it may be interesting knowledge. For example, maybe we can share data between them to predict better, later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, _ = read_csv(\"data/pickups_zone_1_15min.csv\")\n",
    "s17, _ = read_csv(\"data/pickups_zone_17_15min.csv\")\n",
    "\n",
    "np.corrcoef([s1, s17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the areas are well correlated with each other. Can that show, in the pictures above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a more interesting question: are there correlations between a given area, and the other areas in earlier time steps? \n",
    "\n",
    "This is a VERY important one. If you find high correlation, for example, between area 1 at time t, with area 17 at time t-1, then you can use area 17 to predict area 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this, you need to play a little bit with the vectors. Let's call a vector that is shifted in time for 1 time step, a \"lag1\" vector.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s17_lag1=np.array(s17[1:])  #look, it's the SAME vector, except that you start on the second element. Why?\n",
    "#if this is confusing you now, please ask the teacher to explain. \n",
    "s1_trimmed=np.array(s1[:-1]) #With a white board near, it will be easy to understand! ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check those correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef([s1_trimmed, s17_lag1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! Very interesting!! This means that you can use data from these other areas to predict for area 1... This is useful when there is missing data in area 1, for example... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to replicate this same study, for all different areas. To make it quick, you can define a function that receives the area you want to \"predict for\", and the areas you want to use as \"predictors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_correlations(target, predictors, lag=1):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_correlations(s17, [s1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea: check the correlation between the series \"s1\" and the series \"s1_lag1\". This is called the autocorrelation of lag 1 of the time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_correlations(s1, [s1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do this with lag=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_correlations(s1, [s1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the last couple of tasks for today: \n",
    "\n",
    "Create a list, let's call it \"auto_s1\", with the autocorrelations with all lags until 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting diagram is called an autocorrelogram! :-) What do you think? Can you explain what's happening in those bumps?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
