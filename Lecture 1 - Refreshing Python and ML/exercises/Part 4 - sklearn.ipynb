{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshing Python and Machine Learning: Part 4 - sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a Python programmer or you are looking for a robust library you can use to bring machine learning into a production system then a library that you will want to seriously consider is scikit-learn (also known as *sklearn*).\n",
    "\n",
    "Scikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later Matthieu Brucher joined the project and started to use it as apart of his thesis work. In 2010 INRIA got involved and the first public release (v0.1 beta) was published in late January 2010. The project now has more than 30 active contributors and has had paid sponsorship from INRIA, Google, Tinyclues and the Python Software Foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. It is licensed under a permissive simplified BSD license and is distributed under many Linux distributions, encouraging academic and commercial use. The library is built upon the SciPy (Scientific Python) that must be installed before you can use scikit-learn. This stack that includes:\n",
    "\n",
    "- NumPy: Base n-dimensional array package\n",
    "- SciPy: Fundamental library for scientific computing\n",
    "- Matplotlib: Comprehensive 2D/3D plotting\n",
    "- IPython: Enhanced interactive console\n",
    "- Sympy: Symbolic mathematics\n",
    "- Pandas: Data structures and analysis\n",
    "\n",
    "Extensions or modules for SciPy care conventionally named SciKits. As such, the module provides learning algorithms and is named scikit-learn. The vision for the library is a level of robustness and support required for use in production systems. This means a deep focus on concerns such as ease of use, code quality, collaboration, documentation and performance. Although the interface is Python, c-libraries are leverage for performance such as numpy for arrays and matrix operations, LAPACK, LibSVM and the careful use of cython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the features?**\n",
    "\n",
    "The library is focused on modeling data. It is not focused on loading, manipulating and summarizing data. For these features, refer to NumPy and Pandas.\n",
    "\n",
    "Scikit Learn is focused on Machine Learning, e.g data modeling. It is not concerned with the loading, handling, manipulating, and visualizing of data. Thus, it is natural and common practice to use the above libraries, especially NumPy, for those extra steps; they are made for each other! Scikit’s robust set of algorithm offerings includes:\n",
    "- Regression: Fitting linear and non-linear models\n",
    "- Clustering: Unsupervised classification\n",
    "- Decision Trees: Tree induction and pruning for both classification and regression tasks\n",
    "- Neural Networks: End-to-end training for both classification and regression. Layers can be easily defined in a tuple\n",
    "- SVMs: for learning decision boundaries\n",
    "- Naive Bayes: Direct probabilistic modeling\n",
    "\n",
    "Even beyond that, it has some very convenient and advanced functions not commonly offered by other libraries:\n",
    "- Ensemble Methods: Boosting, Bagging, Random Forest, Model voting and averaging\n",
    "- Feature Manipulation: Dimensionality reduction, feature selection, feature analysis\n",
    "- Outlier Detection: For detecting outliers and rejecting noise\n",
    "- Model selection and validation: Cross-validation, hyperparameter tuning, and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful links:**\n",
    "\n",
    "- Documentation: https://scikit-learn.org/stable/documentation.html\n",
    "- User giude: https://scikit-learn.org/stable/user_guide.html\n",
    "- Cheat sheet (basics): https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet\n",
    "- sklearn algorithm cheat sheet: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro to sklearn: Linear regression and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a linear regression exercise to review basic functionality of sklearn. We have the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 30\n",
    "sigma = 0.1\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * sigma\n",
    "X_plot = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build a linear regression model to approximate the unknown true function from noisy samples. This is one of the most common tasks in data analysis. Let's start with a simple model $\\hat{y}=\\beta_0+\\beta_1x$ and try to fit it based on the ordinary least squares estimation. For this one of the most common tasks in data analysis, sklearn has the following class `sklearn.linear_model.LinearRegression`. Let's fit the model and plot results.\n",
    "\n",
    "*NOTE: When working with any package, always remember to check relevant documentation. Just google \"sklearn.linear_model.LinearRegression\" and the first link should be [what you are looking for](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models in sklearn provide `score` method to measure \"goodness of fit\". The default value returned depends on the task. For regression, it's $R^2$ and accuracy is used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root mean squared error is another popular measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_pred-y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sidenote: Check Appendix of this notebook for the explanation of a few popular metrics for regression and classification. The most popular metrics are implemented in the `sklearn.metrics` module. Check the documentation [here](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have calculated these numbers for our model. Are they high or low? To answer this question, we need to have a reference point (a.k.a. a baseline model). For regression, the most \"naive\" baseline is the mean prediction. Even for such a trivial task, the related functionality is already implemented in `sklearn.dummy.DummyRegressor`. There's also `sklearn.dummy.DummyClassifier` class which can be used for naive classification baselines, for example, predicting the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it seems that our linear model performs better than the baseline. Let's try to use some data transformations to enrich our features. Remember that the word \"linear\" refers to the linearity of a model w.r.t. its parameters, not the features (independent variables $x_n$). The most common transformations include power, exp, and log. Here, we will use the power transformation to build a polynomial regression model $\\hat{y}=\\sum_{n=0}^{N}\\beta_nx^{n}$. Let's try a quadratic model ($N=2$) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's try to add even more features by increasing the degree of our polynomial model. Unsurprisingly, there is a function in `sklearn` to get the power-transformed features.\n",
    "\n",
    "*Sidenote: It might be a good idea to skim all the methods implemented in the basic sklearn modules: https://scikit-learn.org/stable/modules/classes.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the goodness of fit, it looks even better! Let's plot the RMSE for the polynomials of different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error decreases with the number of features. However, will this model be able to *generalize*, i.e., make reliable predictions for new (unseen) data? Let's compare the performance of two polynomial models of 2nd and 20th degrees for new data generated in the same way as the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.sort(np.random.rand(n_samples)).reshape(-1, 1)\n",
    "y_new = true_fun(X_new) + np.random.randn(n_samples).reshape(-1, 1) * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_model_RMSE(X, y, X_new, y_new, degree=1):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, the model with 20 features performs much worse. This is a classic example of overfitting. A usual way to address this problem is to use two separate datasets for training and testing a model. We can use `sklearn.model_selection.train_test_split` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the performance of different models on the training and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and find the model which performs the best on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward extension of this split is cross-validation (CV). The most basic CV is a K-fold CV, where the dataset is divided into $K$ chunks (\"folds\"). All but one folds are used for training and the remaining one is used for testing. So the training/test procedure is repeated $K$ times and the evaluation metrics can be averaged over the folds. CV is a more time-consuming procedure but it gives more statistically reliable performance estimation. It is implemented in `sklearn.model_selection.KFold`. For other CV types, check [documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).\n",
    "\n",
    "*Sidenote: Two other important things to know about the train/test splits (and cross-validation) are shuffling and stratification:*\n",
    "- *never shuffle data which is correlated (not i.i.d.) such as time series or spatial data;*\n",
    "- *make sure that the distribution of the train and test data is the same. This is particularly relevant for the classification problems where class distribution can be preserved using stratified CV implemented in `sklearn.model_selection.StratifiedKFold`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If calculating the CV performance metrics is our only goal, we can directly use `sklearn.model_selection.cross_val_score`. This function takes the model, data, scoring function, and CV split. As we want to use RMSE and not the standard scorer, we need to implement this function ourself. **Note, that the score should be higher for a better model in contrast to an error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_scorer(estimator, X, y):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the degree of a polynomial model is not fitted during the model's training and should be defined beforehand (a priori). Such parameters, known as *hyperparameters*, usually define model complexity. Another way to penalize model complexity is regularisation, which corresponds to penalizing the norm of the model's parameters (i.e., forcing many of them to be close to zero). For example, lasso regression (`sklearn.linear_model.Lasso`) is a linear regression where, along with the regular OLS minimization objective, the sum of absolute values (L1-norm) of the regression coefficients is minimized. Ridge regression (`sklearn.linear_model.Ridge`) is another popular model where the sum of squares (L2-norm) of the parameters is penalized. However, these type of models usually has another hyperparameter which defines regularization strength. Let's calculate the CV score of lasso regression for the polynomial of the 10th degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty bad score. Let's try to tune (i.e., find the best value) `alpha` using CV. As it's a very common task, there's a function for this purpose called `sklearn.model_selection.GridSearchCV`. **Note, that the best values of the hyperparameters are selected using the score maximization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `GridSearchCV`, we found a much better value for `alpha`. Let's plot the results for the best lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the lasso coefficients to the coefficients of the original polynomial model of the 10th degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: In this exercise, we used all the data for the hyperparameter tuning. A better way is to tune hyperparameters on the training set only (for example, using CV or train/validation split of the training set). The final performance of different models is estimated on the test set which has not been seen by any model (or nested CV can be used).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably have noticed, we need to run `PolynomialFeatures` transformation every time we use the model. To simplify this, we can use `sklearn.pipeline` which defines a sequance of actions (implementing fit/transform) to be applied to the data. The last step should be an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"polynomial_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"linear_regression\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can be used just like a regular model. When the pipeline's method `fit` is called, all the specified steps use the `fit_transform` method before calling the estimator's `fit`. When the pipeline's method `predict` is called, all the specified steps use the `transform` method before calling the estimator's `predict` (as nothing should be fitted during the prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-cross_val_score(model, X, y, scoring=RMSE_scorer, cv=kf).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of the pipeline steps can be also tuned. To access them, the double underscore '__' should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'polynomial_features__degree': range(1, 10)\n",
    "} # hyperparameters grid for the model to try\n",
    "model_tuned = GridSearchCV(model, parameters, scoring=RMSE_scorer, cv=kf) \n",
    "# scoring and cv are optional, the default values are usually ok\n",
    "model_tuned.fit(X, y)\n",
    "print(\"best_estimator_:\", model_tuned.best_estimator_)\n",
    "print(\"Mean cross-validated score of the best_estimator:\", -model_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common step in machine learning is data normalization. For example, it might be beneficial to have the input features scaled to zero mean and unit variance. For this purpose, `sklearn.preprocessing.StandardScaler` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X_train)\n",
    "X_train = scaler_X.transform(X_train)\n",
    "# which is equivalent to\n",
    "#X_train = scaler_X.fit_transform(X_train)\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)) # 1D arrays are not supported :(\n",
    "#\n",
    "X_test = scaler_X.transform(X_test) # never fit anything to the test set!\n",
    "# fitting the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "# Scaling back\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "RMSE(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to define scaling as a step in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"polynomial_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"linear_regression\", LinearRegression())\n",
    "])\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A few other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a few sklearn examples are provided for you to take a brief look. You're not supposed to memorize them. Check more examples here https://scikit-learn.org/stable/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html*\n",
    "\n",
    "`class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)`\n",
    "\n",
    "**Note that L2-regularization is applied by default, where C defines the inverse regularization strength (~1/alpha)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Logistic regression: Nice tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building A Logistic Regression in Python, Step by Step** \n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Logistic regression: Binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py\n",
    "\n",
    "from sklearn import linear_model\n",
    "from scipy.special import expit\n",
    "\n",
    "# General a toy dataset:s it's just a straight line with some Gaussian noise:\n",
    "xmin, xmax = -5, 5\n",
    "n_samples = 100\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=n_samples)\n",
    "y = (X > 0).astype(np.float)\n",
    "X[X > 0] *= 4\n",
    "X += .3 * np.random.normal(size=n_samples)\n",
    "\n",
    "X = X[:, np.newaxis]\n",
    "\n",
    "# Fit the classifier\n",
    "clf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# and plot the result\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.scatter(X.ravel(), y, color='black', zorder=20)\n",
    "X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "loss = expit(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "plt.plot(X_test, loss, color='red', linewidth=3)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X, y)\n",
    "plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\n",
    "plt.axhline(.5, color='.5')\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('X')\n",
    "plt.xticks(range(-5, 10))\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.ylim(-.25, 1.25)\n",
    "plt.xlim(-4, 10)\n",
    "plt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n",
    "           loc=\"lower right\", fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Logistic regression: Multiclass classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html\n",
    "\n",
    "# Authors: Gael Varoquaux\n",
    "#          Jaques Grobler\n",
    "#          Kevin Hughes\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Create the data\n",
    "\n",
    "e = np.exp(1)\n",
    "np.random.seed(4)\n",
    "\n",
    "\n",
    "def pdf(x):\n",
    "    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)\n",
    "                  + stats.norm(scale=4 / e).pdf(x))\n",
    "\n",
    "y = np.random.normal(scale=0.5, size=(30000))\n",
    "x = np.random.normal(scale=0.5, size=(30000))\n",
    "z = np.random.normal(scale=0.1, size=len(x))\n",
    "\n",
    "density = pdf(x) * pdf(y)\n",
    "pdf_z = pdf(5 * z)\n",
    "\n",
    "density *= pdf_z\n",
    "\n",
    "a = x + y\n",
    "b = 2 * y\n",
    "c = a - b + z\n",
    "\n",
    "norm = np.sqrt(a.var() + b.var())\n",
    "a /= norm\n",
    "b /= norm\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the figures\n",
    "def plot_figs(fig_num, elev, azim):\n",
    "    fig = plt.figure(fig_num, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)\n",
    "\n",
    "    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)\n",
    "    Y = np.c_[a, b, c]\n",
    "\n",
    "    # Using SciPy's SVD, this would be:\n",
    "    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(Y)\n",
    "    pca_score = pca.explained_variance_ratio_\n",
    "    print(pca_score)\n",
    "    V = pca.components_\n",
    "\n",
    "    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T\n",
    "    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]\n",
    "    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]\n",
    "    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]\n",
    "    x_pca_plane.shape = (2, 2)\n",
    "    y_pca_plane.shape = (2, 2)\n",
    "    z_pca_plane.shape = (2, 2)\n",
    "    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "\n",
    "elev = -40\n",
    "azim = -80\n",
    "plot_figs(1, elev, azim)\n",
    "\n",
    "elev = 30\n",
    "azim = 20\n",
    "plot_figs(2, elev, azim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 K-Means tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering with scikit-learn**\n",
    "https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Appendix: Goodness of fit measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Appendix: Goodness of fit measures - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sklearn goodness of fit measures for regression (click on measures to see examples): https://scikit-learn.org/stable/modules/classes.html#regression-metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model that we may choose to formulate should be compared with the current one, not only by observing a predictedVSobserved plot, but also by using the goodness-of-fit measures. They typically summarize the discrepancy between observed values and the values expected under the model in question. Their main categories are presented below:\n",
    "\n",
    "\n",
    "* **Error measures in the estimation period:** root mean squared error, mean absolute error, mean absolute percentage error, mean absolute scaled error, mean error, mean percentage error\n",
    "* **Error measures in the validation period** (if you have done out-of-sample testing)\n",
    "* **Residual diagnostics and goodness-of-fit tests:** plots of actual and predicted values; plots of residuals versus time, versus predicted values, and versus other variables; residual autocorrelation plots, cross-correlation plots, and tests for normally distributed errors; measures of extreme or influential observations; tests for excessive runs, changes in mean, or changes in variance etc.\n",
    "\n",
    "In the current notebook, we will focus on the first category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is any one statistic that normally takes precedence over the others, it is the **root mean squared error (RMSE)**, which is the square root of the mean squared error. This is the statistic whose value is minimized during the parameter estimation process, and it is the statistic that determines the width of the confidence intervals for predictions.\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{\\sum_{t=1}^{n}(\\hat{y_{t}}-y_{t})^2}{n}} $\n",
    "\n",
    "where $\\hat{y_{t}}$ is the predicted or estimated value and $y_{t}$ is the regression's actual observation. Finally the n represents the number of non-missing data points.\n",
    "\n",
    "The function that calculates the RMSE is:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(np.mean((y_pred - y_true)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a datum which ranges from 0 to 1000, an RMSE of 0.7 is small, but if the range goes from 0 to 1, it is not that small anymore. However, although the smaller the RMSE, the better, you can make theoretical claims on levels of the RMSE by knowing what is expected from your dependent variable in your field of research. Keep in mind that you can always normalize the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean absolute error (MAE)** is also measured in the same units as the data, and is usually similar in magnitude to, but slightly smaller than, the root mean squared error.  It is less sensitive to the occasional very large error because it does not square the errors in the calculation. It is the average vertical distance between each point and the Y=X line, which is also known as the One-to-One line.\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{t=1}^{n}\\left |\\hat{y_{t}}-y_{t}\\right |}{n}$$\n",
    "\n",
    "where $\\hat{y_{t}}$ is the predicted or estimated value and $y_{t}$ is the regression's actual observations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mae(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean absolute percentage error (MAPE)** is also often useful for purposes of reporting, because it is expressed in generic percentage terms which will make some kind of sense even to someone who has no idea what constitutes a \"big\" error in terms of dollars spent or widgets sold. The MAPE can only be computed with respect to data that are guaranteed to be strictly positive, so if this statistic is missing from your output where you would normally expect to see it, it’s possible that it has been **suppressed due to negative data values**.\n",
    "\n",
    "$$MAPE = \\frac{100}{N}\\times \\sum_{t=1}^{n}\\left |\\frac{y_{t}-\\hat{y_{t}}}{y_{t}} \\right |$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mape(y_pred, y_true):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in our example, it cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared** is the “percent of variance explained” by the model.  That is, R-squared is the fraction by which the variance of the errors is less than the variance of the dependent variable.  (The latter number would be the error variance for a constant-only model, which merely predicts that every observation will equal the sample mean.)  It is called R-squared because in a simple regression model it is just the square of the correlation between the dependent and independent variables, which is commonly denoted by “r”. \n",
    "\n",
    "If $\\bar{y}$ is the mean of the observed data:\n",
    "\n",
    "$$ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i $$\n",
    "\n",
    "then the variability of the data set can be measured using the following **sum of squares** formulas:\n",
    "\n",
    "* The total sum of squares (proportional to the variable of the data):\n",
    "\n",
    "$$ SS_{tot} = \\sum_{i}^{ }(y_i-\\bar{y})^2 $$\n",
    "\n",
    "* The sum of squares of residuals, also called the residual sum of squares:\n",
    "\n",
    "$$ SS_{res} = \\sum_{i}^{ }(y_i-\\hat{y})^2 $$\n",
    "\n",
    "And the general definition of the coefficient of determination is:\n",
    "\n",
    "$$ R^2 \\equiv 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "An **R-squared** of 1 indicates that the regression line perfectly fits the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def r2(y_pred, y_true):\n",
    "    return max(0, 1 - np.sum((y_true-y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of an **adjusted R-squared** is an attempt to take account of the phenomenon of the **R-squared** automatically and spuriously increasing when extra explanatory variables are added to the model. It is a modification due to Henri Theil of **R-squared** that adjusts for the number of explanatory terms in a model relative to the number of data points. The **adjusted R-squared** can be negative, and its value will always be less than or equal to that of **R-squared**. \n",
    "\n",
    "$$\\bar{R}^2 = R^2 - (1 - R^2)\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "where $p$ is the total number of explanatory variables in the model (not including the constant term), and $n$ is the sample size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def adjustedr2(y_pred, y_true, nvariables):\n",
    "    r_squared = max(0, 1 - np.sum((y_true-y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
    "    return 1 - (1-r_squared)*(len(y_true)-1)/(len(y_true)-nvariables-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the **Pearson’s correlation coefficient** is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. \n",
    "\n",
    "The formula for $\\rho$ is:\n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma _X\\sigma _Y}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $cov$ is the covarianve\n",
    "* $\\sigma _{X}$ is the standard deviation of X\n",
    "* $\\sigma_Y$  is the standard deviation of Y\n",
    "\n",
    "The Pearson’s correlation coefficient can be calculated using the **scipy** package:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "rho2 = pearsonr(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the **numpy** package:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.corrcoef(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you can create a function that calculates all this goodness of fit measures, and reuse is as often as you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Appendix: Goodness of fit measures - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sklearn goodness of fit measures for classification (click on measures to see examples): https://scikit-learn.org/stable/modules/classes.html#classification-metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification model like Logistic Regression will output a probability number between 0 and 1 instead of the desired output of actual target variable like Yes/No, etc. The next logical step is to translate this probability number into the target/dependent variable in the model and test the accuracy of the model. To understand the implication of translating the probability number, let’s understand few basic concepts relating to evaluating a classification model with the help of an example given below.\n",
    "\n",
    "**Goal:** Create a classification model that predicts fraud transactions\n",
    "\n",
    "**Output:** Transactions that are predicted to be Fraud and Non-Fraud\n",
    "\n",
    "**Testing:** Comparing the predicted result with the actual results\n",
    "\n",
    "**Dataset:** Number of Observations: 1 million; Fraud : 100; Non-Fraud: 999,900\n",
    "\n",
    "\n",
    "Assuming you were able to translate the output of your model to Fraud/Non-Fraud, the predicted result could be compared to actual result and summarized as follows:\n",
    "\n",
    "a) **True Positives (TP):** Observations where the actual and predicted transactions were fraud\n",
    " \n",
    "b) **True Negatives (TN):** Observations where the actual and predicted transactions weren’t fraud\n",
    "\n",
    "c) **False Positives (FP):** Observations where the actual transactions weren’t fraud but predicted to be fraud\n",
    "\n",
    "d) **False Negatives (FN):** Observations where the actual transactions were fraud but weren’t predicted to be fraud\n",
    "\n",
    "From the class notebook, you have already learnt the **Confusion Matrix**, which is a very popular way to represent the summarized findings:\n",
    "\n",
    "\n",
    "<img src=\"ConfusionMatrix.png\" style=\"max-width:100%; width: 40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity, Specificity, Precision\n",
    "\n",
    "The performance of our classification function is also evaluated using the statistical measures **sensitivity**, **specificity** and **precision**.\n",
    "\n",
    "* **Sensitivity, recall, hit rate, or true positive rate (TPR)** measures the proportion of positives that are correctly identified as such.\n",
    "\n",
    "Sensitivity = No. of True Positives / (No. of True Positives + No. of False Negatives)\n",
    "\n",
    "$$ Sensitivity (TPR)= \\frac{TP}{(TP + FN)} $$\n",
    "\n",
    "* **Specificity or true negative rate (TNR)** measures the proportion of negatives that are correctly identified as such.\n",
    "\n",
    "Specificity = No. of True Negatives / (No. of True Negatives + No. of False Positives)\n",
    "\n",
    "$$ Specificity (TNR)= \\frac{TN}{(TN + FP)}  $$\n",
    "\n",
    "* **Precision or positive predictive value (PPV)** is calculated as:\n",
    "\n",
    "Precision = No. of True Positives / (No. of True Positives + No. of False Positives)\n",
    "\n",
    "$$ Precision (PPV) = \\frac{TP}{(TP + FP)} $$\n",
    "\n",
    "*Since the formula does not contain FN and TN, Precision may give you a biased result, especially for imbalanced classes.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-measure and Kappa\n",
    "\n",
    "A measure that combines precision and recall is the harmonic mean of precision and recall, the traditional **F-measure or balanced F-score**:\n",
    "\n",
    "$$ F = 2 \\cdot \\frac{precision\\cdot recall}{precision + recall} $$\n",
    "\n",
    "$$ F = 2 \\cdot \\frac{PPV\\cdot TPR}{PPV + TPR} $$\n",
    "\n",
    "This measure is approximately the average of the two when they are close, and is more generally the harmonic mean, which, for the case of two numbers, coincides with the square of the geometric mean divided by the arithmetic mean. \n",
    "\n",
    "The **Kappa statistic (or value)** is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). The kappa statistic is used not only to evaluate a single classifier, but also to evaluate classifiers amongst themselves. In addition, it takes into account random chance (agreement with a random classifier), which generally means it is less misleading than simply using accuracy as a metric (an Observed Accuracy of 80% is a lot less impressive with an Expected Accuracy of 75% versus an Expected Accuracy of 50%). Computation of Observed Accuracy and Expected Accuracy is integral to comprehension of the kappa statistic, and is most easily illustrated through use of a confusion matrix. \n",
    "\n",
    "Lets begin with a simple confusion matrix from a simple binary classification of Cats and Dogs:\n",
    "\n",
    "$$\\begin{vmatrix}\n",
    "  & Cats  & Dogs \\\\ \n",
    "Cats & 10 & 7\\\\ \n",
    " Dogs & 5  & 8\n",
    "\\end{vmatrix}$$\n",
    "\n",
    "From the confusion matrix we can see there are 30 instances total $(10 + 7 + 5 + 8 = 30)$. The **observed accuracy** of our classifier is the number of instances that were classified correctly throughout the entire confusion matrix. For this confusion matrix, this would be $0.6 ((10 + 8) / 30 = 0.6)$. \n",
    "\n",
    "Before we get to the equation for the kappa statistic, one more value is needed: the **Expected Accuracy.** This value is defined as the accuracy that any random classifier would be expected to achieve based on the confusion matrix. The Expected Accuracy is directly related to the number of instances of each class (Cats and Dogs), along with the number of instances that the machine learning classifier agreed with the ground truth label. \n",
    "\n",
    "To calculate Expected Accuracy for our confusion matrix, first multiply the marginal frequency of Cats for one \"rater\" by the marginal frequency of Cats for the second \"rater\", and divide by the total number of instances. The marginal frequency for a certain class by a certain \"rater\" is just the sum of all instances the \"rater\" indicated were that class. In our case, $15 (10 + 5 = 15)$ instances were labeled as Cats according to *ground truth*, and $17 (10 + 7 = 17)$ instances were classified as Cats by the machine learning classifier. This results in a value of $8.5 (15 * 17 / 30 = 8.5)$. \n",
    "This is then done for the second class as well (and can be repeated for each additional class if there are more than 2). $15 (10 + 5 = 15)$ instances were labeled as Dogs according to ground truth, and $13 (10 + 7 = 17) $instances were classified as Dogs by the machine learning classifier. This results in a value of $6.5 (15 * 13 / 30 = 6.5)$. The final step is to add all these values together, and finally divide again by the total number of instances, resulting in an **Expected Accuracy** of $0.5 ((8.5 + 6.5) / 30 = 0.5)$. \n",
    "\n",
    "In our example, the Expected Accuracy turned out to be 50%, as will always be the case when either \"rater\" classifies each class with the same frequency in a binary classification (both Cats and Dogs contained 15 instances according to ground truth labels in our confusion matrix).\n",
    "\n",
    "The kappa statistic can then be calculated using both the **Observed Accuracy** (0.60) and the **Expected Accuracy** (0.50) and the formula:\n",
    "\n",
    "$$ Kappa = \\frac{ObservedAccuracy - ExpectedAccuracy}{1 - ExpectedAccuracy} =  \\frac{0.60 - 0.50}{1 - 0.50} = 0.20$$\n",
    "\n",
    "There is not a standardized interpretation of the kappa statistic. According to Wikipedia (citing their paper), Landis and Koch considers 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect. Fleiss considers kappas > 0.75 as excellent, 0.40-0.75 as fair to good, and < 0.40 as poor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
